Q.1 What is classic machine and adaptive machines ?
  [a]. Classic machines
      -> A machine which is made for specific task, and performs exately based on its task definition is called as classic/Non-adaptive machines
      -> It cannot be adaptive with respect to the new environment

  [b]. Adaptive machines
      -> It also takes "feedback" as input
      -> Adopts changes in environment
      -> Feedback = peace of information about the changes in environment
      -> It has a little bit capacity to think like human brain

-> 'Features' are the columns of the dataset
-> It is necessary to identify necessary features for model training, because in dataset, some features doesn't contribute to get efficient prediction value

                                                 ------------------------------
                                                 | TYPES OF FEATURE SELECTION |
                                                 ------------------------------ 
                                                              |
                                                              |
                                                              |
  ---------------------------------------------------------------------------------------------------------------------------
  |                                                           |                                                             |                                                        
  |                                                           |                                                             |    
FILTER METHOD                                          WRAPPER METHOD                                                EMBEDDED METHODS

1.Variance threshold                                   1.Recursive feature elimination                               1.Lasso regression(L1 reg.)                                
2.Correlation matrix with heatmap                      2.Sequential feature selection                                2.Ridge regression(L2 reg.)
3.chi-squared test                                     3.Genetic algorithms                                          3.Elastic net
4.ANOVA (Analysis for variance)                                                                                      4.Tree based method
5.IG(information gain) method

-> Each filtering methods try to find out that, how much individual features are corelated to the output feature
-> Does individual features has any relation with dependent output feature or not ?

> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <

Q.2 What does these methods does ?
  1. Filter method
    Elimination and feature selection

  2. Wrapper method
    Combines the output feature + combination of indipendent features, and generates 'n' number of models based combinations, and checks the accuracy of each model

  3. Embedded method
    Similar as embedded method

> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <

Q.3 What is PCA(Principal component analysis)
    -> For overfitting problem
    -> It trys to convert high dimentionality into low dimentionality
    -> It fully depends on the view point, how you see, from where you see the graph
    -> Number of PC can be <= Number of features
    -> In case of multipal PCs, 1st PC will have higher prioarity and than compare with eachother for better decision
    -> All PCs must be orthogonal to each other(Meaning: All PCs must be indipendent to each other)

> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <

Q.4 What is R-squared
    SSres = SUM(Yi - Yhat) ** 2 ||  SUM(actual value of Y - predicted value of Y) ** 2
    SStot = SUM(Yi = Yavg) ** 2 ||  SUM(actual value of Y - average value of Y) ** 2

    (R ** 2) = 1 - (SSres/SStot)

    -> Usually, in most of cases (R ** 2) < SStot
    -> Lies between 0 and 1
    -> THUMB RULE FOR IT....(based on industry use cases)
       [a]. 1.0 : Perfect fit (suspecious)
       [b]. ~0.9 : Very good
       [c]. <0.7 : Not great
       [d]. <0.4 : Terrible
       [e]. <0 : Model makes no sence for this data

    Adjusted R-squared = 1 - (1 - (R ** 2)) * (n - 1/n - k - 1)
        In which:
          n : Sample size
          k : Number of independent veriables



