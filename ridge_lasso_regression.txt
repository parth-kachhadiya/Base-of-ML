[A]. RIDGE Regression
    
   âœ’ï¸ It gives same importance to all fetures
   âœ’ï¸ Reduces the size of coefficient to reduce the chances of 'overfitting'

   Parameters....

   [a]. alpha : [1.0] (regularization strength)
        âœ’ï¸ It controls how much regularization you apply
        âœ’ï¸ Low alpha : Almost no regularization (behave like standard LinearRegression)
	âœ’ï¸ High alpha : Lots of regularization, model shrinks the coefficients aggressively
   
   [b]. fit_intercept : [True]
        âœ’ï¸ Determines whether to include an intercept or not

   [c]. solver 
	âœ’ï¸ Which algorithm is used to find the optimal solution of Ridge Regression
 
 	solvers.
		âž¡ï¸ 'auto' :
			âœ’ï¸ automatically selects the best solver
		âž¡ï¸ 'svd' :
			âœ’ï¸ Works well, when number of fetures is smaller than number of samples
		âž¡ï¸ 'cholesky' :
			âœ’ï¸ Fast and works well on smaller datasets
		âž¡ï¸ 'lsqr' :
			âœ’ï¸ An iterative solver, userful if your data is larger
		âž¡ï¸ 'sag' & 'saga' :
			âœ’ï¸ Efficient for large dataset, assumes that the features are already scaled

    [d]. max_iter [Depends on solver]
	 âœ’ï¸ Maximum number of iteration the solver will take
	 âœ’ï¸ If your model is taking too long to converge(of find the best solution), you can increase this number

    [e]. tol : [1e-3] (tolerance)
	 âœ’ï¸ Sets the tolerance for stopping criteria.
	 âœ’ï¸ It means that when the model has found a solution that doesn't improve much, it will stop early,
             saving time.
	 âœ’ï¸ In simple words, if model is improving by only tiny amounts, it stops looking for better solution.
    
    [f]. random_state : [None]
    
    [g]. copy_x : [True]
 
    [h]. positive : [False]

> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <> - <

[B]. LASSO Regression

    âœ’ï¸ It forces you to prioritize by shrinking some coefficients down to zero.
    âœ’ï¸ It can remove unnecessary variables entirely, helping feature selection & simplifying the model

    [a]. alpha : [1.0]

    [b]. fit_intercept : [True]

    [c]. precompute : 
         âœ’ï¸ Whether to precompute the gram matrix(A matrix derived from the dot products of the features) to speed up computations.
         âœ’ï¸ Useful in large datasets where repeated calculations can slow down the process.
         âœ’ï¸ It's like a faster route for model to find the solution.
	 âœ’ï¸ If dataset is large, it saves time by precomputing some stuff in advance.
	     ðŸ‘‰ If precompute=True, lasso will compute and store gram matrix(Which is like a big cheat sheet of all feature relationship) before starting optimization
	         it makes process faster, specially when number of rows in dataset is larger
	     ðŸ‘‰ If precompute=False, It won't calculate gram matrix & will rely on recalculating necessary information during each steps of optimization
		 this can be slower for large dataset.	
             ðŸ‘‰ use 'True' for larger dataset & 'False' for smaller dataset where no need to speedup.

    [d]. max_iter : [1000]
         âœ’ï¸ Maximum number of iterations allowed to find the optimal solution.
         âœ’ï¸ This is the effort limit, If model takes too long to find the best solution, you can increase that number

    [e]. tol : [1e-4]

    [f]. selection : [cyclic]
	 âœ’ï¸ How to choose the features to update at each iteration.
 	 âœ’ï¸ How the model decides to tweak things
              âž¡ï¸ 'cyclic'
			âœ’ï¸ Update features one by one.
              âž¡ï¸ 'random'
			âœ’ï¸ Update features in random order, which can sometimes converge faster.

    [g]. random_state : [None]

    [h]. copy_x : [True]

    [i]. positive : [False]
